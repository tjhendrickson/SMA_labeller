
############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-01-23 12:51:07.211407: do_dummy_2d_data_aug: False
2025-01-23 12:51:07.215308: Using splits from existing split file: /home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/splits_final.json
2025-01-23 12:51:07.219730: The split file contains 5 splits.
2025-01-23 12:51:07.221214: Desired fold for training: 4
2025-01-23 12:51:07.222503: This split has 53 training and 13 validation cases.
using pin_memory on device 0
using pin_memory on device 0
2025-01-23 12:51:17.812001: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [160, 160, 96], 'median_image_size_in_voxels': [255.0, 255.0, 176.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset502_MICCAI2012_and_316_CBIT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [255, 255, 176], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1713.0, 'mean': 784.4163208007812, 'median': 787.0, 'min': 1.0, 'percentile_00_5': 128.0, 'percentile_99_5': 1335.0, 'std': 241.68751525878906}}} 

2025-01-23 12:51:19.518692: unpacking dataset...
Exception in background worker 7:
 [Errno 2] No such file or directory: '/home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/nnUNetPlans_3d_fullres/1002.pkl'
Exception in background worker 10:
 [Errno 2] No such file or directory: '/home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/nnUNetPlans_3d_fullres/1019.pkl'
Exception in background worker 6:
 [Errno 2] No such file or directory: '/home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/nnUNetPlans_3d_fullres/1012.pkl'
2025-01-23 12:51:27.275899: unpacking done...
2025-01-23 12:51:27.294231: Unable to plot network architecture: nnUNet_compile is enabled!
2025-01-23 12:51:27.317835: 
2025-01-23 12:51:27.319441: Epoch 0
2025-01-23 12:51:27.321009: Current learning rate: 0.01
2025-01-23 12:53:10.246221: train_loss 0.0006
2025-01-23 12:53:10.248657: val_loss -0.1781
2025-01-23 12:53:10.250385: Pseudo dice [np.float32(0.0)]
2025-01-23 12:53:10.252176: Epoch time: 102.93 s
2025-01-23 12:53:10.253829: Yayy! New best EMA pseudo Dice: 0.0
2025-01-23 12:53:12.324035: 
2025-01-23 12:53:12.325897: Epoch 1
2025-01-23 12:53:12.327698: Current learning rate: 0.00999
2025-01-23 12:53:42.157003: train_loss -0.3731
2025-01-23 12:53:42.159404: val_loss -0.4608
2025-01-23 12:53:42.162635: Pseudo dice [np.float32(0.5373)]
2025-01-23 12:53:42.165169: Epoch time: 29.83 s
2025-01-23 12:53:42.166874: Yayy! New best EMA pseudo Dice: 0.053700000047683716
2025-01-23 12:53:44.131525: 
2025-01-23 12:53:44.133434: Epoch 2
2025-01-23 12:53:44.134878: Current learning rate: 0.00998
2025-01-23 12:54:13.993366: train_loss -0.4683
2025-01-23 12:54:13.996342: val_loss -0.5554
2025-01-23 12:54:13.998066: Pseudo dice [np.float32(0.6149)]
2025-01-23 12:54:13.999722: Epoch time: 29.86 s
2025-01-23 12:54:14.001338: Yayy! New best EMA pseudo Dice: 0.10980000346899033
2025-01-23 12:54:15.998766: 
2025-01-23 12:54:16.000623: Epoch 3
2025-01-23 12:54:16.002501: Current learning rate: 0.00997
2025-01-23 12:54:46.178797: train_loss -0.5084
2025-01-23 12:54:46.183728: val_loss -0.6024
2025-01-23 12:54:46.186687: Pseudo dice [np.float32(0.6718)]
2025-01-23 12:54:46.188169: Epoch time: 30.18 s
2025-01-23 12:54:46.189738: Yayy! New best EMA pseudo Dice: 0.16599999368190765
2025-01-23 12:54:47.827871: 
2025-01-23 12:54:47.858384: Epoch 4
2025-01-23 12:54:47.860183: Current learning rate: 0.00996
2025-01-23 12:55:18.237136: train_loss -0.5516
2025-01-23 12:55:18.239631: val_loss -0.604
2025-01-23 12:55:18.241628: Pseudo dice [np.float32(0.6663)]
2025-01-23 12:55:18.243354: Epoch time: 30.41 s
2025-01-23 12:55:18.245288: Yayy! New best EMA pseudo Dice: 0.21610000729560852
2025-01-23 12:55:20.007875: 
2025-01-23 12:55:20.009525: Epoch 5
2025-01-23 12:55:20.011057: Current learning rate: 0.00995
2025-01-23 12:55:50.491997: train_loss -0.5674
2025-01-23 12:55:50.494446: val_loss -0.5477
2025-01-23 12:55:50.496280: Pseudo dice [np.float32(0.6016)]
2025-01-23 12:55:50.498063: Epoch time: 30.49 s
2025-01-23 12:55:50.499787: Yayy! New best EMA pseudo Dice: 0.25459998846054077
2025-01-23 12:55:52.501258: 
2025-01-23 12:55:52.503359: Epoch 6
2025-01-23 12:55:52.505034: Current learning rate: 0.00995
2025-01-23 12:56:22.564302: train_loss -0.5935
2025-01-23 12:56:22.566422: val_loss -0.6211
2025-01-23 12:56:22.568068: Pseudo dice [np.float32(0.684)]
2025-01-23 12:56:22.569756: Epoch time: 30.06 s
2025-01-23 12:56:22.571308: Yayy! New best EMA pseudo Dice: 0.29760000109672546
2025-01-23 12:56:24.431506: 
2025-01-23 12:56:24.433457: Epoch 7
2025-01-23 12:56:24.435017: Current learning rate: 0.00994
2025-01-23 12:56:54.544214: train_loss -0.5855
2025-01-23 12:56:54.546013: val_loss -0.6787
2025-01-23 12:56:54.547395: Pseudo dice [np.float32(0.7409)]
2025-01-23 12:56:54.548750: Epoch time: 30.11 s
2025-01-23 12:56:54.550106: Yayy! New best EMA pseudo Dice: 0.3418999910354614
2025-01-23 12:56:56.184517: 
2025-01-23 12:56:56.186748: Epoch 8
2025-01-23 12:56:56.188851: Current learning rate: 0.00993
2025-01-23 12:57:26.930049: train_loss -0.6097
2025-01-23 12:57:26.931873: val_loss -0.6505
2025-01-23 12:57:26.933339: Pseudo dice [np.float32(0.7195)]
2025-01-23 12:57:26.934667: Epoch time: 30.75 s
2025-01-23 12:57:26.936400: Yayy! New best EMA pseudo Dice: 0.3797000050544739
2025-01-23 12:57:28.892436: 
2025-01-23 12:57:28.894420: Epoch 9
2025-01-23 12:57:28.896338: Current learning rate: 0.00992
2025-01-23 12:57:58.835487: train_loss -0.5953
2025-01-23 12:57:58.837597: val_loss -0.6743
2025-01-23 12:57:58.839345: Pseudo dice [np.float32(0.7255)]
2025-01-23 12:57:58.841086: Epoch time: 29.94 s
2025-01-23 12:57:58.842780: Yayy! New best EMA pseudo Dice: 0.4142000079154968
2025-01-23 12:58:01.178816: 
2025-01-23 12:58:01.180632: Epoch 10
2025-01-23 12:58:01.182287: Current learning rate: 0.00991
2025-01-23 12:58:30.878016: train_loss -0.6215
2025-01-23 12:58:30.888411: val_loss -0.6633
2025-01-23 12:58:30.890322: Pseudo dice [np.float32(0.7637)]
2025-01-23 12:58:30.891986: Epoch time: 29.7 s
2025-01-23 12:58:30.893455: Yayy! New best EMA pseudo Dice: 0.44920000433921814
2025-01-23 12:58:32.610157: 
2025-01-23 12:58:32.630445: Epoch 11
2025-01-23 12:58:32.632205: Current learning rate: 0.0099
2025-01-23 12:59:02.487473: train_loss -0.6117
2025-01-23 12:59:02.489758: val_loss -0.6782
2025-01-23 12:59:02.492284: Pseudo dice [np.float32(0.7429)]
2025-01-23 12:59:02.494022: Epoch time: 29.88 s
2025-01-23 12:59:02.495675: Yayy! New best EMA pseudo Dice: 0.47859999537467957
2025-01-23 12:59:04.331444: 
2025-01-23 12:59:04.349152: Epoch 12
2025-01-23 12:59:04.351132: Current learning rate: 0.00989
2025-01-23 12:59:34.090724: train_loss -0.6386
2025-01-23 12:59:34.092892: val_loss -0.6886
2025-01-23 12:59:34.094508: Pseudo dice [np.float32(0.7766)]
2025-01-23 12:59:34.096114: Epoch time: 29.76 s
2025-01-23 12:59:34.097712: Yayy! New best EMA pseudo Dice: 0.508400022983551
2025-01-23 12:59:35.981593: 
2025-01-23 12:59:35.983655: Epoch 13
2025-01-23 12:59:35.985426: Current learning rate: 0.00988
2025-01-23 13:00:05.795153: train_loss -0.6191
2025-01-23 13:00:05.797494: val_loss -0.6589
2025-01-23 13:00:05.799151: Pseudo dice [np.float32(0.7189)]
2025-01-23 13:00:05.800776: Epoch time: 29.82 s
2025-01-23 13:00:05.802381: Yayy! New best EMA pseudo Dice: 0.5293999910354614
2025-01-23 13:00:07.640114: 
2025-01-23 13:00:07.642312: Epoch 14
2025-01-23 13:00:07.644484: Current learning rate: 0.00987
2025-01-23 13:00:37.580216: train_loss -0.6204
2025-01-23 13:00:37.582331: val_loss -0.7029
2025-01-23 13:00:37.584048: Pseudo dice [np.float32(0.7595)]
2025-01-23 13:00:37.585752: Epoch time: 29.94 s
2025-01-23 13:00:37.587494: Yayy! New best EMA pseudo Dice: 0.5523999929428101
2025-01-23 13:00:39.499773: 
2025-01-23 13:00:39.536510: Epoch 15
2025-01-23 13:00:39.538314: Current learning rate: 0.00986
2025-01-23 13:01:09.103939: train_loss -0.6169
2025-01-23 13:01:09.106197: val_loss -0.7195
2025-01-23 13:01:09.107717: Pseudo dice [np.float32(0.7748)]
2025-01-23 13:01:09.109431: Epoch time: 29.61 s
2025-01-23 13:01:09.111058: Yayy! New best EMA pseudo Dice: 0.5746999979019165
2025-01-23 13:01:10.931166: 
2025-01-23 13:01:10.933050: Epoch 16
2025-01-23 13:01:10.934799: Current learning rate: 0.00986
2025-01-23 13:01:40.920021: train_loss -0.6406
2025-01-23 13:01:40.922670: val_loss -0.6743
2025-01-23 13:01:40.924598: Pseudo dice [np.float32(0.7673)]
2025-01-23 13:01:40.926431: Epoch time: 29.99 s
2025-01-23 13:01:40.928340: Yayy! New best EMA pseudo Dice: 0.5939000248908997
2025-01-23 13:01:42.928349: 
2025-01-23 13:01:42.950911: Epoch 17
2025-01-23 13:01:42.952757: Current learning rate: 0.00985
2025-01-23 13:02:12.774192: train_loss -0.647
2025-01-23 13:02:12.776788: val_loss -0.6707
2025-01-23 13:02:12.778556: Pseudo dice [np.float32(0.7744)]
2025-01-23 13:02:12.780179: Epoch time: 29.85 s
2025-01-23 13:02:12.781837: Yayy! New best EMA pseudo Dice: 0.6119999885559082
2025-01-23 13:02:14.712954: 
2025-01-23 13:02:14.714924: Epoch 18
2025-01-23 13:02:14.716681: Current learning rate: 0.00984
2025-01-23 13:02:44.553077: train_loss -0.6482
2025-01-23 13:02:44.555274: val_loss -0.7247
2025-01-23 13:02:44.557237: Pseudo dice [np.float32(0.7853)]
2025-01-23 13:02:44.559011: Epoch time: 29.84 s
2025-01-23 13:02:44.560799: Yayy! New best EMA pseudo Dice: 0.6292999982833862
2025-01-23 13:02:48.122682: 
2025-01-23 13:02:48.124505: Epoch 19
2025-01-23 13:02:48.126037: Current learning rate: 0.00983
2025-01-23 13:03:18.383019: train_loss -0.6432
2025-01-23 13:03:18.385150: val_loss -0.6946
2025-01-23 13:03:18.386810: Pseudo dice [np.float32(0.7789)]
2025-01-23 13:03:18.388476: Epoch time: 30.26 s
2025-01-23 13:03:18.389854: Yayy! New best EMA pseudo Dice: 0.6442999839782715
2025-01-23 13:03:20.328598: 
2025-01-23 13:03:20.330310: Epoch 20
2025-01-23 13:03:20.331913: Current learning rate: 0.00982

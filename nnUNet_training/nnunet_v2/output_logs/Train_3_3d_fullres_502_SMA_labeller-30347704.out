
############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-01-23 12:51:07.363076: do_dummy_2d_data_aug: False
2025-01-23 12:51:07.365491: Using splits from existing split file: /home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/splits_final.json
2025-01-23 12:51:07.367421: The split file contains 5 splits.
2025-01-23 12:51:07.369120: Desired fold for training: 3
2025-01-23 12:51:07.370595: This split has 53 training and 13 validation cases.
using pin_memory on device 0
using pin_memory on device 0
2025-01-23 12:51:14.374557: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [160, 160, 96], 'median_image_size_in_voxels': [255.0, 255.0, 176.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset502_MICCAI2012_and_316_CBIT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [255, 255, 176], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1713.0, 'mean': 784.4163208007812, 'median': 787.0, 'min': 1.0, 'percentile_00_5': 128.0, 'percentile_99_5': 1335.0, 'std': 241.68751525878906}}} 

2025-01-23 12:51:16.584960: unpacking dataset...
Exception in background worker 4:
 [Errno 2] No such file or directory: '/home/cconelea/shared/projects/316_CBIT/BIDS_output/code/SMA_labeller/data/nnUNet/nnunet_v2/nnUNet_preprocessed/Dataset502_MICCAI2012_and_316_CBIT/nnUNetPlans_3d_fullres/1038.pkl'
2025-01-23 12:51:24.823699: unpacking done...
2025-01-23 12:51:24.850734: Unable to plot network architecture: nnUNet_compile is enabled!
2025-01-23 12:51:24.904349: 
2025-01-23 12:51:24.905986: Epoch 0
2025-01-23 12:51:24.907569: Current learning rate: 0.01
2025-01-23 12:53:10.289352: train_loss 0.0415
2025-01-23 12:53:10.291861: val_loss -0.0558
2025-01-23 12:53:10.293511: Pseudo dice [np.float32(0.0)]
2025-01-23 12:53:10.295279: Epoch time: 105.39 s
2025-01-23 12:53:10.297048: Yayy! New best EMA pseudo Dice: 0.0
2025-01-23 12:53:12.273901: 
2025-01-23 12:53:12.287393: Epoch 1
2025-01-23 12:53:12.289126: Current learning rate: 0.00999
2025-01-23 12:53:42.199321: train_loss -0.1432
2025-01-23 12:53:42.201839: val_loss -0.3083
2025-01-23 12:53:42.203414: Pseudo dice [np.float32(0.0042)]
2025-01-23 12:53:42.205601: Epoch time: 29.93 s
2025-01-23 12:53:42.207454: Yayy! New best EMA pseudo Dice: 0.00039999998989515007
2025-01-23 12:53:44.227143: 
2025-01-23 12:53:44.229697: Epoch 2
2025-01-23 12:53:44.231357: Current learning rate: 0.00998
2025-01-23 12:54:14.101994: train_loss -0.4166
2025-01-23 12:54:14.104176: val_loss -0.4819
2025-01-23 12:54:14.105827: Pseudo dice [np.float32(0.5491)]
2025-01-23 12:54:14.107498: Epoch time: 29.88 s
2025-01-23 12:54:14.108874: Yayy! New best EMA pseudo Dice: 0.05530000105500221
2025-01-23 12:54:15.716498: 
2025-01-23 12:54:15.718618: Epoch 3
2025-01-23 12:54:15.720484: Current learning rate: 0.00997
2025-01-23 12:54:46.265037: train_loss -0.492
2025-01-23 12:54:46.267312: val_loss -0.5499
2025-01-23 12:54:46.269028: Pseudo dice [np.float32(0.6194)]
2025-01-23 12:54:46.270746: Epoch time: 30.55 s
2025-01-23 12:54:46.272313: Yayy! New best EMA pseudo Dice: 0.11169999837875366
2025-01-23 12:54:48.256414: 
2025-01-23 12:54:48.258629: Epoch 4
2025-01-23 12:54:48.260284: Current learning rate: 0.00996
2025-01-23 12:55:18.307989: train_loss -0.5318
2025-01-23 12:55:18.310610: val_loss -0.4757
2025-01-23 12:55:18.312457: Pseudo dice [np.float32(0.5381)]
2025-01-23 12:55:18.315614: Epoch time: 30.05 s
2025-01-23 12:55:18.317145: Yayy! New best EMA pseudo Dice: 0.1543000042438507
2025-01-23 12:55:20.182132: 
2025-01-23 12:55:20.183985: Epoch 5
2025-01-23 12:55:20.185620: Current learning rate: 0.00995
2025-01-23 12:55:50.455011: train_loss -0.5765
2025-01-23 12:55:50.457453: val_loss -0.5718
2025-01-23 12:55:50.459289: Pseudo dice [np.float32(0.6313)]
2025-01-23 12:55:50.461203: Epoch time: 30.27 s
2025-01-23 12:55:50.462988: Yayy! New best EMA pseudo Dice: 0.20200000703334808
2025-01-23 12:55:52.017140: 
2025-01-23 12:55:52.019133: Epoch 6
2025-01-23 12:55:52.020778: Current learning rate: 0.00995
2025-01-23 12:56:22.463575: train_loss -0.5913
2025-01-23 12:56:22.466310: val_loss -0.5047
2025-01-23 12:56:22.468101: Pseudo dice [np.float32(0.5889)]
2025-01-23 12:56:22.469849: Epoch time: 30.45 s
2025-01-23 12:56:22.471626: Yayy! New best EMA pseudo Dice: 0.24070000648498535
2025-01-23 12:56:24.125797: 
2025-01-23 12:56:24.127583: Epoch 7
2025-01-23 12:56:24.129485: Current learning rate: 0.00994
2025-01-23 12:56:54.142818: train_loss -0.594
2025-01-23 12:56:54.145212: val_loss -0.5798
2025-01-23 12:56:54.146993: Pseudo dice [np.float32(0.634)]
2025-01-23 12:56:54.148695: Epoch time: 30.02 s
2025-01-23 12:56:54.150527: Yayy! New best EMA pseudo Dice: 0.2800000011920929
2025-01-23 12:56:56.105210: 
2025-01-23 12:56:56.107078: Epoch 8
2025-01-23 12:56:56.108803: Current learning rate: 0.00993
2025-01-23 12:57:26.016444: train_loss -0.6283
2025-01-23 12:57:26.018741: val_loss -0.5658
2025-01-23 12:57:26.020545: Pseudo dice [np.float32(0.6383)]
2025-01-23 12:57:26.021889: Epoch time: 29.91 s
2025-01-23 12:57:26.023606: Yayy! New best EMA pseudo Dice: 0.3158999979496002
2025-01-23 12:57:27.651047: 
2025-01-23 12:57:27.675228: Epoch 9
2025-01-23 12:57:27.677048: Current learning rate: 0.00992
2025-01-23 12:57:57.421215: train_loss -0.6224
2025-01-23 12:57:57.423788: val_loss -0.5742
2025-01-23 12:57:57.425716: Pseudo dice [np.float32(0.6409)]
2025-01-23 12:57:57.427710: Epoch time: 29.77 s
2025-01-23 12:57:57.429718: Yayy! New best EMA pseudo Dice: 0.3483999967575073
2025-01-23 12:58:00.114669: 
2025-01-23 12:58:00.127150: Epoch 10
2025-01-23 12:58:00.128963: Current learning rate: 0.00991
2025-01-23 12:58:29.987007: train_loss -0.6249
2025-01-23 12:58:29.990284: val_loss -0.5272
2025-01-23 12:58:29.992243: Pseudo dice [np.float32(0.6087)]
2025-01-23 12:58:29.994128: Epoch time: 29.87 s
2025-01-23 12:58:29.996054: Yayy! New best EMA pseudo Dice: 0.37439998984336853
2025-01-23 12:58:31.631623: 
2025-01-23 12:58:31.633618: Epoch 11
2025-01-23 12:58:31.635228: Current learning rate: 0.0099
2025-01-23 12:59:01.447906: train_loss -0.625
2025-01-23 12:59:01.465069: val_loss -0.5892
2025-01-23 12:59:01.467555: Pseudo dice [np.float32(0.67)]
2025-01-23 12:59:01.469054: Epoch time: 29.82 s
2025-01-23 12:59:01.470490: Yayy! New best EMA pseudo Dice: 0.40400001406669617
2025-01-23 12:59:03.353330: 
2025-01-23 12:59:03.355365: Epoch 12
2025-01-23 12:59:03.357312: Current learning rate: 0.00989
2025-01-23 12:59:32.899151: train_loss -0.6316
2025-01-23 12:59:32.901966: val_loss -0.5746
2025-01-23 12:59:32.903888: Pseudo dice [np.float32(0.6591)]
2025-01-23 12:59:32.905774: Epoch time: 29.55 s
2025-01-23 12:59:32.907595: Yayy! New best EMA pseudo Dice: 0.429500013589859
2025-01-23 12:59:35.010873: 
2025-01-23 12:59:35.025988: Epoch 13
2025-01-23 12:59:35.027769: Current learning rate: 0.00988
2025-01-23 13:00:04.810199: train_loss -0.6265
2025-01-23 13:00:04.812643: val_loss -0.6074
2025-01-23 13:00:04.814720: Pseudo dice [np.float32(0.662)]
2025-01-23 13:00:04.816674: Epoch time: 29.8 s
2025-01-23 13:00:04.818683: Yayy! New best EMA pseudo Dice: 0.4526999890804291
2025-01-23 13:00:06.778076: 
2025-01-23 13:00:06.780101: Epoch 14
2025-01-23 13:00:06.782121: Current learning rate: 0.00987
2025-01-23 13:00:36.886954: train_loss -0.6541
2025-01-23 13:00:36.889490: val_loss -0.6178
2025-01-23 13:00:36.891275: Pseudo dice [np.float32(0.6744)]
2025-01-23 13:00:36.892871: Epoch time: 30.11 s
2025-01-23 13:00:36.894306: Yayy! New best EMA pseudo Dice: 0.4749000072479248
2025-01-23 13:00:38.687953: 
2025-01-23 13:00:38.689986: Epoch 15
2025-01-23 13:00:38.691722: Current learning rate: 0.00986
2025-01-23 13:01:08.792952: train_loss -0.6634
2025-01-23 13:01:08.795529: val_loss -0.6264
2025-01-23 13:01:08.797593: Pseudo dice [np.float32(0.6848)]
2025-01-23 13:01:08.799621: Epoch time: 30.11 s
2025-01-23 13:01:08.801550: Yayy! New best EMA pseudo Dice: 0.4959000051021576
2025-01-23 13:01:10.797415: 
2025-01-23 13:01:10.799368: Epoch 16
2025-01-23 13:01:10.801056: Current learning rate: 0.00986
2025-01-23 13:01:41.141249: train_loss -0.6631
2025-01-23 13:01:41.148364: val_loss -0.6028
2025-01-23 13:01:41.149837: Pseudo dice [np.float32(0.6882)]
2025-01-23 13:01:41.151192: Epoch time: 30.35 s
2025-01-23 13:01:41.152370: Yayy! New best EMA pseudo Dice: 0.5151000022888184
2025-01-23 13:01:42.845939: 
2025-01-23 13:01:42.847981: Epoch 17
2025-01-23 13:01:42.849594: Current learning rate: 0.00985
2025-01-23 13:02:12.676595: train_loss -0.656
2025-01-23 13:02:12.679141: val_loss -0.6137
2025-01-23 13:02:12.681160: Pseudo dice [np.float32(0.6804)]
2025-01-23 13:02:12.683127: Epoch time: 29.83 s
2025-01-23 13:02:12.684718: Yayy! New best EMA pseudo Dice: 0.5315999984741211
2025-01-23 13:02:14.751682: 
2025-01-23 13:02:14.753829: Epoch 18
2025-01-23 13:02:14.755741: Current learning rate: 0.00984
2025-01-23 13:02:44.726887: train_loss -0.6577
2025-01-23 13:02:44.728884: val_loss -0.6067
2025-01-23 13:02:44.730649: Pseudo dice [np.float32(0.6857)]
2025-01-23 13:02:44.732376: Epoch time: 29.98 s
2025-01-23 13:02:44.734129: Yayy! New best EMA pseudo Dice: 0.5471000075340271
2025-01-23 13:02:49.143085: 
2025-01-23 13:02:49.144633: Epoch 19
2025-01-23 13:02:49.146154: Current learning rate: 0.00983
2025-01-23 13:03:19.117455: train_loss -0.67
2025-01-23 13:03:19.119773: val_loss -0.5923
2025-01-23 13:03:19.121681: Pseudo dice [np.float32(0.6604)]
2025-01-23 13:03:19.123510: Epoch time: 29.98 s
2025-01-23 13:03:19.125409: Yayy! New best EMA pseudo Dice: 0.5583999752998352
2025-01-23 13:03:21.152524: 
2025-01-23 13:03:21.154312: Epoch 20
2025-01-23 13:03:21.156098: Current learning rate: 0.00982
